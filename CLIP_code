#Sample CLIP Code# 

import torch
import torch.nn.functional as F
from transformers import CLIPModel, CLIPProcessor

# Load pre-trained CLIP model and processor
model_name = "openai/clip-vit-base-patch32"
model = CLIPModel.from_pretrained(model_name)
processor = CLIPProcessor.from_pretrained(model_name)

# Sample images and text (replace with your actual data loading)
images = [torch.randn(3, 224, 224), torch.randn(3, 224, 224)]
texts = ["a cat sitting on a mat", "a dog running in the park"]

# Process inputs
inputs = processor(text=texts, images=images, return_tensors="pt", padding=True)

# Get embeddings
outputs = model(**inputs)
image_embeddings = outputs.image_embeds
text_embeddings = outputs.text_embeds

# Normalize embeddings
image_embeddings = image_embeddings / image_embeddings.norm(dim=1, keepdim=True)
text_embeddings = text_embeddings / text_embeddings.norm(dim=1, keepdim=True)

# Calculate cosine similarity
similarity_matrix = text_embeddings @ image_embeddings.T

# Temperature scaling (learnable parameter or fixed value)
temperature = 0.07
similarity_matrix = similarity_matrix / temperature

# Create labels for contrastive loss
labels = torch.arange(len(images))

# Calculate contrastive loss
loss_fct = torch.nn.CrossEntropyLoss()
loss_img = loss_fct(similarity_matrix, labels)
loss_txt = loss_fct(similarity_matrix.T, labels)
loss = (loss_img + loss_txt) / 2

print("Contrastive Loss:", loss.item())
